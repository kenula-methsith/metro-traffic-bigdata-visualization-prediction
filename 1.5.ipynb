{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757843ad-2ec1-4a82-9af7-b4cee75bf87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "+-------+-------+-------+-------+----------+------------+--------------------+---------------+--------------+-----+\n",
      "|holiday|   temp|rain_1h|snow_1h|clouds_all|weather_main| weather_description|      date_time|traffic_volume|count|\n",
      "+-------+-------+-------+-------+----------+------------+--------------------+---------------+--------------+-----+\n",
      "|   None| 292.84|    0.0|    0.0|         1|       Clear|        sky is clear|6/30/2017 10:00|          4638|    2|\n",
      "|   None| 286.29|    0.0|    0.0|         1|       Clear|        sky is clear|9/30/2015 19:00|          3679|    2|\n",
      "|   None| 296.95|    0.0|    0.0|         1|Thunderstorm|        thunderstorm|6/30/2017 13:00|          5263|    2|\n",
      "|   None| 287.86|    0.0|    0.0|         0|       Clear|        Sky is Clear|9/29/2016 19:00|          3435|    2|\n",
      "|   None| 288.89|    0.0|    0.0|         1|       Clear|        sky is clear|9/29/2017 19:00|          4498|    2|\n",
      "|   None|289.775|    0.0|    0.0|        56|      Clouds|       broken clouds|9/21/2016 15:00|          5365|    2|\n",
      "|   None| 295.01|    0.0|    0.0|        40|      Clouds|    scattered clouds|6/21/2017 11:00|          4808|    2|\n",
      "|   None| 295.77|    0.0|    0.0|        75|      Clouds|       broken clouds|6/30/2017 12:00|          5229|    2|\n",
      "|   None| 267.89|    0.0|    0.0|        90|        Snow|          light snow|12/6/2016 18:00|          4520|    2|\n",
      "|   None|279.287|    0.0|    0.0|        56|      Clouds|       broken clouds|10/7/2016 18:00|          4642|    2|\n",
      "|   None| 266.22|    0.0|    0.0|         1|       Clear|        sky is clear|12/5/2017 18:00|          3936|    2|\n",
      "|   None| 289.06|    0.0|    0.0|        90|      Clouds|     overcast clouds| 6/1/2016 10:00|          4831|    2|\n",
      "|   None| 280.68|    0.0|    0.0|        90|      Clouds|     overcast clouds|9/29/2018 19:00|          3818|    2|\n",
      "|   None| 294.52|    0.0|    0.0|         1|       Clear|        sky is clear|6/30/2017 11:00|          4725|    2|\n",
      "|   None| 254.22|    0.0|    0.0|         1|       Clear|        sky is clear|12/19/2016 0:00|           420|    2|\n",
      "|   None| 278.72|    0.0|    0.0|        90|        Mist|                mist| 11/5/2017 1:00|           629|    2|\n",
      "|   None| 278.72|    0.0|    0.0|        90|     Drizzle|light intensity d...| 11/5/2017 1:00|           629|    2|\n",
      "+-------+-------+-------+-------+----------+------------+--------------------+---------------+--------------+-----+\n",
      "\n",
      "Duplicate successfully removed\n",
      "Date time oder successfully\n",
      "Duplicate Rows:\n",
      "+-------+----+-------+-------+----------+------------+-------------------+---------+--------------+-----+\n",
      "|holiday|temp|rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time|traffic_volume|count|\n",
      "+-------+----+-------+-------+----------+------------+-------------------+---------+--------------+-----+\n",
      "+-------+----+-------+-------+----------+------------+-------------------+---------+--------------+-----+\n",
      "\n",
      "Null Counts Per Column:\n",
      "+-------+----+-------+-------+----------+------------+-------------------+---------+--------------+\n",
      "|holiday|temp|rain_1h|snow_1h|clouds_all|weather_main|weather_description|date_time|traffic_volume|\n",
      "+-------+----+-------+-------+----------+------------+-------------------+---------+--------------+\n",
      "|      0|   0|      0|      0|         0|           0|                  0|        0|             0|\n",
      "+-------+----+-------+-------+----------+------------+-------------------+---------+--------------+\n",
      "\n",
      "Cleaned and transformed dataset saved successfully at C:\\Users\\thinu\\Desktop\\KENULA\\top-up\\data\\cleaned_transformed_dataset2.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, isnan\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Step 1: Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Cleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "file_path = r'C:\\Users\\thinu\\Desktop\\KENULA\\top-up\\data\\Metro_Interstate_Traffic_Volume.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Step 4: Data Cleaning\n",
    "\n",
    "# 4.1: Identify and remove duplicate rows\n",
    "# Step 3: Identify duplicate rows\n",
    "duplicates = data.groupBy(data.columns).count().filter(\"count > 1\")\n",
    "print(\"Duplicate Rows:\")\n",
    "duplicates.show()\n",
    "\n",
    "# Step 4: Remove duplicate rows\n",
    "data = data.dropDuplicates()\n",
    "print(\"Duplicate successfully removed\")\n",
    "\n",
    "# Order by date \n",
    "data = data.orderBy(\"date_time\")\n",
    "print(\"Date time oder successfully\")\n",
    "\n",
    "# Step 3: Identify duplicate rows\n",
    "duplicates = data.groupBy(data.columns).count().filter(\"count > 1\")\n",
    "print(\"Duplicate Rows:\")\n",
    "duplicates.show()\n",
    "\n",
    "# 4.2: Identify and handle null or missing values\n",
    "\n",
    "# Separate numeric and non-numeric columns\n",
    "numeric_columns = [field.name for field in data.schema.fields if str(field.dataType) in ['DoubleType', 'IntegerType', 'FloatType']]\n",
    "non_numeric_columns = [field.name for field in data.schema.fields if field.name not in numeric_columns]\n",
    "\n",
    "# Check nulls for all columns, and isnan only for numeric columns\n",
    "null_counts = data.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in non_numeric_columns\n",
    "] + [\n",
    "    count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in numeric_columns\n",
    "])\n",
    "print(\"Null Counts Per Column:\")\n",
    "null_counts.show()\n",
    "\n",
    "# Step 5: Export the cleaned and transformed dataset to CSV\n",
    "output_path = r'C:\\Users\\thinu\\Desktop\\KENULA\\top-up\\data\\cleaned_transformed_dataset2.csv'\n",
    "\n",
    "try:\n",
    "    data.toPandas().to_csv(output_path, index=False)\n",
    "    print(f\"Cleaned and transformed dataset saved successfully at {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving dataset: {e}\")\n",
    "\n",
    "# Step 6: Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd666274-7dad-4beb-89c4-0b15bf847e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
